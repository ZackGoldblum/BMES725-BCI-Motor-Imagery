{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd0bd9b6b4994d3bb2f3181bdd53871f722d7a4323dd9d419f0561d20da3c614246",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "bd9b6b4994d3bb2f3181bdd53871f722d7a4323dd9d419f0561d20da3c614246"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "#sys.path.insert(1, r\"C:\\Users\\Zack\\OneDrive\\BCI\\arl-eegmodels-master\")\n",
    "#from EEGModels import ShallowConvNet\n",
    "\n",
    "# main directory\n",
    "main_dir = os.getcwd()\n",
    "# eeg data directory\n",
    "eeg_dir = os.path.join(main_dir, \"eeg_data\")\n",
    "# eeg validation data directory\n",
    "eeg_validation_dir = os.path.join(main_dir, \"eeg_data_validation\")\n",
    "\n",
    "MI_CLASSES = [\"left\", \"right\", \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_sec(eeg_data, num_sec=5, samp_freq=250):\n",
    "    one_sec_data = []\n",
    "    for i in range(num_sec):\n",
    "        eeg_data_i = eeg_data[i*samp_freq:(i+1)*samp_freq]\n",
    "        one_sec_data.append(eeg_data_i)\n",
    "\n",
    "    return one_sec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_dir):\n",
    "    training_data_dict = {}\n",
    "\n",
    "    for mi_class in MI_CLASSES:\n",
    "        if mi_class not in training_data_dict:\n",
    "            training_data_dict[mi_class] = []\n",
    "\n",
    "        mi_class_dir = os.path.join(data_dir, mi_class)\n",
    "        for filename in os.listdir(mi_class_dir):\n",
    "            if \"filtered\" in filename:\n",
    "                eeg_data = np.loadtxt(os.path.join(mi_class_dir, filename), delimiter=',')  # (250, 8) eeg data\n",
    "                #eeg_dataT = np.transpose(eeg_data)                                      # (8, 250) eeg data\n",
    "                one_sec_list = to_one_sec(eeg_data, num_sec=5, samp_freq=250)\n",
    "                for i in range(len(one_sec_list)):  # for each 1 second trial\n",
    "                    one_sec_data = one_sec_list[i] # (250, 8) eeg data\n",
    "                    one_sec_dataT = np.transpose(one_sec_data)  # (8, 250) eeg data\n",
    "                    training_data_dict[mi_class].append(one_sec_dataT)  \n",
    "                #training_data_dict[mi_class].append(eeg_dataT)  \n",
    "\n",
    "    print(\"Trials per class: \" + str(int(len(os.listdir(mi_class_dir))/3)) + \"\\n\")\n",
    "    session_count = [len(training_data_dict[mi_class]) for mi_class in MI_CLASSES]\n",
    "    print(f\"One sec trial count:\\nLeft: {session_count[0]}, Right: {session_count[1]}, None: {session_count[2]}\\n\")\n",
    "\n",
    "    for mi_class in MI_CLASSES:\n",
    "        np.random.shuffle(training_data_dict[mi_class])  # randomize session order\n",
    "        training_data_dict[mi_class] = training_data_dict[mi_class][:min(session_count)]  # use min session count number of sessions\n",
    "\n",
    "    # creating X, y \n",
    "    labeled_data = []\n",
    "    for mi_class in MI_CLASSES:\n",
    "        for data in training_data_dict[mi_class]:\n",
    "            if mi_class == \"left\":\n",
    "                labeled_data.append([data, [1, 0, 0]])\n",
    "            elif mi_class == \"right\":\n",
    "                labeled_data.append([data, [0, 1, 0]])\n",
    "            elif mi_class == \"none\":\n",
    "                labeled_data.append([data, [0, 0, 1]])\n",
    "\n",
    "    np.random.shuffle(labeled_data)\n",
    "    \n",
    "    return labeled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trials per class: 15\n\nOne sec trial count:\nLeft: 75, Right: 75, None: 75\n\nX shape: (num trials, num channels, num samples)\ny shape: (num trials, one hot labels)\n\nCreating labeled data...\nlabeled_data_X: (225, 8, 250)\nlabeled_data_y: (225, 3)\n\nCreating testing data...\ntrain_X: (135, 8, 250)\ntrain_y: (135, 3)\n\nCreating training data...\ntest_X: (45, 8, 250)\ntest_y: (45, 3)\n\nCreating validation data...\nvalidation_X: (45, 8, 250)\nvalidation_y: (45, 3)\n\n"
     ]
    }
   ],
   "source": [
    "labeled_data = create_data(data_dir=eeg_dir)\n",
    "\n",
    "print(\"X shape: (num trials, num channels, num samples)\")\n",
    "print(\"y shape: (num trials, one hot labels)\\n\")\n",
    "\n",
    "print(\"Creating labeled data...\")\n",
    "labeled_data_X = []\n",
    "labeled_data_y = []\n",
    "\n",
    "for X, y in labeled_data:\n",
    "    labeled_data_X.append(X)\n",
    "    labeled_data_y.append(y)\n",
    "    \n",
    "print(\"labeled_data_X: \" + str(np.shape(labeled_data_X)))\n",
    "print(\"labeled_data_y: \" + str(np.shape(labeled_data_y)) + \"\\n\")\n",
    "\n",
    "print(\"Creating testing data...\")\n",
    "train_X = np.array(labeled_data_X[0:9*3*5])\n",
    "train_y = np.array(labeled_data_y[0:9*3*5])\n",
    "print(\"train_X: \" + str(np.shape(train_X)))\n",
    "print(\"train_y: \" + str(np.shape(train_y)) + \"\\n\")\n",
    "\n",
    "print(\"Creating training data...\")\n",
    "test_X = np.array(labeled_data_X[9*3*5:12*3*5])\n",
    "test_y = np.array(labeled_data_y[9*3*5:12*3*5])\n",
    "print(\"test_X: \" + str(np.shape(test_X)))\n",
    "print(\"test_y: \" + str(np.shape(test_y)) + \"\\n\")\n",
    "\n",
    "print(\"Creating validation data...\")\n",
    "validation_X = np.array(labeled_data_X[12*3*5:])\n",
    "validation_y = np.array(labeled_data_y[12*3*5:])\n",
    "print(\"validation_X: \" + str(np.shape(validation_X)))\n",
    "print(\"validation_y: \" + str(np.shape(validation_y)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, AveragePooling2D\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def square(x):\n",
    "        return K.square(x)\n",
    "\n",
    "    def log(x):\n",
    "        return K.log(K.clip(x, min_value = 1e-7, max_value = 10000))   \n",
    "\n",
    "    def ShallowConvNet(nb_classes, Chans = 8, Samples = 250, dropoutRate = 0.5):\n",
    "        # start the model\n",
    "        input_main   = Input((Chans, Samples, 1))\n",
    "        block1       = Conv2D(40, (1, 25), \n",
    "                                        input_shape=(Chans, Samples, 1),\n",
    "                                        kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\n",
    "        block1       = Conv2D(40, (Chans, 1), use_bias=False, \n",
    "                                kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "        block1       = BatchNormalization(epsilon=1e-05, momentum=0.1)(block1)\n",
    "        block1       = Activation(square)(block1)\n",
    "        block1       = AveragePooling2D(pool_size=(1, 75), strides=(1, 15))(block1)\n",
    "        block1       = Activation(log)(block1)\n",
    "        block1       = Dropout(dropoutRate)(block1)\n",
    "        flatten      = Flatten()(block1)\n",
    "        dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "        softmax      = Activation('softmax')(dense)\n",
    "\n",
    "        return Model(inputs=input_main, outputs=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 250, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 226, 40)        1040      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 226, 40)        12800     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1, 226, 40)        160       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 226, 40)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 11, 40)         0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 11, 40)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 11, 40)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 440)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 1323      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 15,323\n",
      "Trainable params: 15,243\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ShallowConvNet(nb_classes=3, Chans=8, Samples=250)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer =\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 30ms/step - loss: 1.3689 - accuracy: 0.5437 - val_loss: 0.9189 - val_accuracy: 0.6444\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.8346 - accuracy: 0.6323 - val_loss: 1.7720 - val_accuracy: 0.5111\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.6808 - accuracy: 0.6817 - val_loss: 0.9965 - val_accuracy: 0.6889\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.9942 - accuracy: 0.6231 - val_loss: 0.8397 - val_accuracy: 0.4667\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.9656 - accuracy: 0.6806 - val_loss: 1.1878 - val_accuracy: 0.5111\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.7346 - accuracy: 0.7647 - val_loss: 1.7585 - val_accuracy: 0.3556\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.6712 - accuracy: 0.7756 - val_loss: 1.0543 - val_accuracy: 0.3778\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.8338 - accuracy: 0.6824 - val_loss: 1.3591 - val_accuracy: 0.4444\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.6463 - accuracy: 0.6911 - val_loss: 1.0801 - val_accuracy: 0.4444\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.5498 - accuracy: 0.7856 - val_loss: 1.1911 - val_accuracy: 0.4667\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.4025 - accuracy: 0.8115 - val_loss: 1.5872 - val_accuracy: 0.4222\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.6443 - accuracy: 0.7767 - val_loss: 1.9109 - val_accuracy: 0.4222\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.7675 - accuracy: 0.7393 - val_loss: 0.3920 - val_accuracy: 0.7556\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5887 - accuracy: 0.7397 - val_loss: 1.0745 - val_accuracy: 0.6889\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.6331 - accuracy: 0.8212 - val_loss: 1.8681 - val_accuracy: 0.3333\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.3988 - accuracy: 0.8718 - val_loss: 3.0311 - val_accuracy: 0.1556\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4688 - accuracy: 0.8119 - val_loss: 1.9349 - val_accuracy: 0.5556\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 0.4482 - accuracy: 0.7921 - val_loss: 0.7466 - val_accuracy: 0.6889\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.5101 - accuracy: 0.8445 - val_loss: 0.9572 - val_accuracy: 0.5556\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5148 - accuracy: 0.7941 - val_loss: 1.5178 - val_accuracy: 0.6889\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.3499 - accuracy: 0.8804 - val_loss: 1.6798 - val_accuracy: 0.5556\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.4324 - accuracy: 0.8784 - val_loss: 2.3587 - val_accuracy: 0.3333\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.3369 - accuracy: 0.9100 - val_loss: 0.7832 - val_accuracy: 0.7778\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.6995 - accuracy: 0.8365 - val_loss: 0.3641 - val_accuracy: 0.8444\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4648 - accuracy: 0.8326 - val_loss: 1.1980 - val_accuracy: 0.6889\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.2417 - accuracy: 0.8992 - val_loss: 0.8530 - val_accuracy: 0.7111\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.2936 - accuracy: 0.9211 - val_loss: 2.8573 - val_accuracy: 0.4667\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.3093 - accuracy: 0.9044 - val_loss: 0.1952 - val_accuracy: 0.8667\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.3245 - accuracy: 0.9011 - val_loss: 0.7056 - val_accuracy: 0.5778\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.2524 - accuracy: 0.9078 - val_loss: 1.4724 - val_accuracy: 0.4222\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.2111 - accuracy: 0.9478 - val_loss: 1.1130 - val_accuracy: 0.7111\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.2824 - accuracy: 0.8734 - val_loss: 0.3659 - val_accuracy: 0.7556\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.2120 - accuracy: 0.9413 - val_loss: 1.3381 - val_accuracy: 0.6222\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.2825 - accuracy: 0.9166 - val_loss: 0.4377 - val_accuracy: 0.8000\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.3321 - accuracy: 0.8893 - val_loss: 0.8909 - val_accuracy: 0.4667\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.3468 - accuracy: 0.8584 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.2098 - accuracy: 0.9413 - val_loss: 1.6080 - val_accuracy: 0.7333\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.3173 - accuracy: 0.9212 - val_loss: 1.3808 - val_accuracy: 0.6444\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.4012 - accuracy: 0.8820 - val_loss: 0.4837 - val_accuracy: 0.8667\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.8263 - accuracy: 0.8834 - val_loss: 0.4711 - val_accuracy: 0.8000\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.3279 - accuracy: 0.9130 - val_loss: 2.4542 - val_accuracy: 0.4000\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.2625 - accuracy: 0.9378 - val_loss: 0.1516 - val_accuracy: 0.9778\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.1714 - accuracy: 0.9521 - val_loss: 0.8383 - val_accuracy: 0.7333\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.1956 - accuracy: 0.9264 - val_loss: 0.5986 - val_accuracy: 0.7333\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.1772 - accuracy: 0.9241 - val_loss: 0.4183 - val_accuracy: 0.8889\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.2473 - accuracy: 0.9030 - val_loss: 1.1869 - val_accuracy: 0.5778\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.2102 - accuracy: 0.9309 - val_loss: 2.4119 - val_accuracy: 0.4889\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.3183 - accuracy: 0.9015 - val_loss: 1.0213 - val_accuracy: 0.6667\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.2423 - accuracy: 0.9313 - val_loss: 1.4257 - val_accuracy: 0.6222\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.1369 - accuracy: 0.9318 - val_loss: 0.9630 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 10\n",
    "fitted_model = model.fit(train_X, train_y, batch_size=batch_size, epochs=50, validation_data=(validation_X, validation_y))\n",
    "#fitted_model = model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.8667\n",
      "\n",
      "Classification accuracy: 86.67\n"
     ]
    }
   ],
   "source": [
    "#probs = model.predict(test_X)\n",
    "#preds = probs.argmax(axis = -1)  \n",
    "#acc = np.mean(preds == test_y.argmax(axis=-1))\n",
    "#print(\"Classification accuracy: %f \" % (acc))\n",
    "\n",
    "score = model.evaluate(test_X, test_y, batch_size=batch_size)\n",
    "classification_accuracy = round(score[1]*100, 2)\n",
    "print(\"\\nClassification accuracy: \" + str(classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: models/86.67-acc_50-epochs_10-batchsize_1622249313.model\\assets\n",
      "\n",
      "Model saved.\n",
      "models/86.67-acc_50-epochs_10-batchsize_1622249313.model\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"models/{classification_accuracy}-acc_{epochs}-epochs_{batch_size}-batchsize_{int(time.time())}.model\"\n",
    "model.save(model_name)\n",
    "print(\"\\nModel saved.\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}