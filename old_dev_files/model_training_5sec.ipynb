{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd0bd9b6b4994d3bb2f3181bdd53871f722d7a4323dd9d419f0561d20da3c614246",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "bd9b6b4994d3bb2f3181bdd53871f722d7a4323dd9d419f0561d20da3c614246"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "#sys.path.insert(1, r\"C:\\Users\\Zack\\OneDrive\\BCI\\arl-eegmodels-master\")\n",
    "#from EEGModels import ShallowConvNet\n",
    "\n",
    "# main directory\n",
    "main_dir = os.getcwd()\n",
    "# eeg data directory\n",
    "eeg_dir = os.path.join(main_dir, \"eeg_data\")\n",
    "# eeg validation data directory\n",
    "eeg_validation_dir = os.path.join(main_dir, \"eeg_data_validation\")\n",
    "\n",
    "MI_CLASSES = [\"left\", \"right\", \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_sec(eeg_data, num_sec=5, samp_freq=250):\n",
    "    one_sec_data = []\n",
    "    for i in range(num_sec):\n",
    "        eeg_data_i = eeg_data[i*samp_freq:(i+1)*samp_freq]\n",
    "        one_sec_data.append(eeg_data_i)\n",
    "\n",
    "    return one_sec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_dir):\n",
    "    training_data_dict = {}\n",
    "\n",
    "    for mi_class in MI_CLASSES:\n",
    "        if mi_class not in training_data_dict:\n",
    "            training_data_dict[mi_class] = []\n",
    "\n",
    "        mi_class_dir = os.path.join(data_dir, mi_class)\n",
    "        for filename in os.listdir(mi_class_dir):\n",
    "            if \"filtered\" in filename:\n",
    "                eeg_data = np.loadtxt(os.path.join(mi_class_dir, filename), delimiter=',')  # (250, 8) eeg data\n",
    "                eeg_dataT = np.transpose(eeg_data)                                      # (8, 250) eeg data\n",
    "                #one_sec_list = to_one_sec(eeg_data, num_sec=5, samp_freq=250)\n",
    "                #for i in range(len(one_sec_list)):  # for each 1 second trial\n",
    "                #    one_sec_data = one_sec_list[i] # (250, 8) eeg data\n",
    "                #    one_sec_dataT = np.transpose(one_sec_data)  # (8, 250) eeg data\n",
    "                #    training_data_dict[mi_class].append(one_sec_dataT)  \n",
    "                training_data_dict[mi_class].append(eeg_dataT)  \n",
    "\n",
    "    print(\"Trials per class: \" + str(int(len(os.listdir(mi_class_dir))/3)) + \"\\n\")\n",
    "    session_count = [len(training_data_dict[mi_class]) for mi_class in MI_CLASSES]\n",
    "    #print(f\"One sec trial count:\\nLeft: {session_count[0]}, Right: {session_count[1]}, None: {session_count[2]}\\n\")\n",
    "\n",
    "    for mi_class in MI_CLASSES:\n",
    "        np.random.shuffle(training_data_dict[mi_class])  # randomize session order\n",
    "        training_data_dict[mi_class] = training_data_dict[mi_class][:min(session_count)]  # use min session count number of sessions\n",
    "\n",
    "    # creating X, y \n",
    "    labeled_data = []\n",
    "    for mi_class in MI_CLASSES:\n",
    "        for data in training_data_dict[mi_class]:\n",
    "            if mi_class == \"left\":\n",
    "                labeled_data.append([data, [1, 0, 0]])\n",
    "            elif mi_class == \"right\":\n",
    "                labeled_data.append([data, [0, 1, 0]])\n",
    "            elif mi_class == \"none\":\n",
    "                labeled_data.append([data, [0, 0, 1]])\n",
    "\n",
    "    np.random.shuffle(labeled_data)\n",
    "    \n",
    "    return labeled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trials per class: 15\n\nX shape: (num trials, num channels, num samples)\ny shape: (num trials, one hot labels)\n\nCreating labeled data...\nlabeled_data_X: (45, 8, 1250)\nlabeled_data_y: (45, 3)\n\nCreating testing data...\ntrain_X: (27, 8, 1250)\ntrain_y: (27, 3)\n\nCreating training data...\ntest_X: (9, 8, 1250)\ntest_y: (9, 3)\n\nCreating validation data...\nvalidation_X: (9, 8, 1250)\nvalidation_y: (9, 3)\n\n"
     ]
    }
   ],
   "source": [
    "labeled_data = create_data(data_dir=eeg_dir)\n",
    "\n",
    "print(\"X shape: (num trials, num channels, num samples)\")\n",
    "print(\"y shape: (num trials, one hot labels)\\n\")\n",
    "\n",
    "print(\"Creating labeled data...\")\n",
    "labeled_data_X = []\n",
    "labeled_data_y = []\n",
    "\n",
    "for X, y in labeled_data:\n",
    "    labeled_data_X.append(X)\n",
    "    labeled_data_y.append(y)\n",
    "    \n",
    "print(\"labeled_data_X: \" + str(np.shape(labeled_data_X)))\n",
    "print(\"labeled_data_y: \" + str(np.shape(labeled_data_y)) + \"\\n\")\n",
    "\n",
    "print(\"Creating testing data...\")\n",
    "train_X = np.array(labeled_data_X[0:9*3])\n",
    "train_y = np.array(labeled_data_y[0:9*3])\n",
    "print(\"train_X: \" + str(np.shape(train_X)))\n",
    "print(\"train_y: \" + str(np.shape(train_y)) + \"\\n\")\n",
    "\n",
    "print(\"Creating training data...\")\n",
    "test_X = np.array(labeled_data_X[9*3:12*3])\n",
    "test_y = np.array(labeled_data_y[9*3:12*3])\n",
    "print(\"test_X: \" + str(np.shape(test_X)))\n",
    "print(\"test_y: \" + str(np.shape(test_y)) + \"\\n\")\n",
    "\n",
    "print(\"Creating validation data...\")\n",
    "validation_X = np.array(labeled_data_X[12*3:])\n",
    "validation_y = np.array(labeled_data_y[12*3:])\n",
    "print(\"validation_X: \" + str(np.shape(validation_X)))\n",
    "print(\"validation_y: \" + str(np.shape(validation_y)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, AveragePooling2D\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def square(x):\n",
    "        return K.square(x)\n",
    "\n",
    "    def log(x):\n",
    "        return K.log(K.clip(x, min_value = 1e-7, max_value = 10000))   \n",
    "\n",
    "    def ShallowConvNet(nb_classes, Chans = 8, Samples = 250, dropoutRate = 0.5):\n",
    "        # start the model\n",
    "        input_main   = Input((Chans, Samples, 1))\n",
    "        block1       = Conv2D(40, (1, 25), \n",
    "                                        input_shape=(Chans, Samples, 1),\n",
    "                                        kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\n",
    "        block1       = Conv2D(40, (Chans, 1), use_bias=False, \n",
    "                                kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "        block1       = BatchNormalization(epsilon=1e-05, momentum=0.1)(block1)\n",
    "        block1       = Activation(square)(block1)\n",
    "        block1       = AveragePooling2D(pool_size=(1, 75), strides=(1, 15))(block1)\n",
    "        block1       = Activation(log)(block1)\n",
    "        block1       = Dropout(dropoutRate)(block1)\n",
    "        flatten      = Flatten()(block1)\n",
    "        dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "        softmax      = Activation('softmax')(dense)\n",
    "\n",
    "        return Model(inputs=input_main, outputs=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 8, 1250, 1)]      0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 8, 1226, 40)       1040      \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 1, 1226, 40)       12800     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 1, 1226, 40)       160       \n_________________________________________________________________\nactivation_8 (Activation)    (None, 1, 1226, 40)       0         \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 1, 77, 40)         0         \n_________________________________________________________________\nactivation_9 (Activation)    (None, 1, 77, 40)         0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 1, 77, 40)         0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 3080)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 9243      \n_________________________________________________________________\nactivation_10 (Activation)   (None, 3)                 0         \n=================================================================\nTotal params: 23,243\nTrainable params: 23,163\nNon-trainable params: 80\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ShallowConvNet(nb_classes=3, Chans=8, Samples=250)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer =\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 157ms/step - loss: 2.6329 - accuracy: 0.2917 - val_loss: 4.8172 - val_accuracy: 0.2222\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.2235 - accuracy: 0.6463 - val_loss: 6.4491 - val_accuracy: 0.1111\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 1.0858 - accuracy: 0.7144 - val_loss: 3.4486 - val_accuracy: 0.2222\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 2.2722 - accuracy: 0.3722 - val_loss: 3.0750 - val_accuracy: 0.6667\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.9762 - accuracy: 0.3727 - val_loss: 2.9135 - val_accuracy: 0.6667\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.9291 - accuracy: 0.5653 - val_loss: 1.3661 - val_accuracy: 0.7778\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 1.7621 - accuracy: 0.6954 - val_loss: 3.1382 - val_accuracy: 0.3333\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4410 - accuracy: 0.7889 - val_loss: 1.5710 - val_accuracy: 0.7778\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.6497 - accuracy: 0.8074 - val_loss: 2.3553 - val_accuracy: 0.7778\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.7242 - accuracy: 0.7023 - val_loss: 1.9658 - val_accuracy: 0.7778\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 2.2398 - accuracy: 0.6963 - val_loss: 1.3838 - val_accuracy: 0.7778\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.5730 - accuracy: 0.5463 - val_loss: 3.6618 - val_accuracy: 0.6667\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 1.1033 - accuracy: 0.6088 - val_loss: 3.7347 - val_accuracy: 0.4444\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 1.1191 - accuracy: 0.6958 - val_loss: 5.6523 - val_accuracy: 0.6667\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.6477 - accuracy: 0.8324 - val_loss: 5.1667 - val_accuracy: 0.5556\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 1.4814 - accuracy: 0.5463 - val_loss: 7.4714 - val_accuracy: 0.1111\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 1.5618 - accuracy: 0.5653 - val_loss: 3.1698 - val_accuracy: 0.4444\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.7487 - accuracy: 0.7269 - val_loss: 0.6003 - val_accuracy: 0.8889\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.9888 - accuracy: 0.6644 - val_loss: 1.0965 - val_accuracy: 0.5556\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.6018 - accuracy: 0.7514 - val_loss: 1.3062 - val_accuracy: 0.5556\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 1.0690 - accuracy: 0.5588 - val_loss: 5.8361 - val_accuracy: 0.3333\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 1.9726 - accuracy: 0.4778 - val_loss: 3.4210 - val_accuracy: 0.3333\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3745 - accuracy: 0.9255 - val_loss: 5.3086 - val_accuracy: 0.1111\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 1.4363 - accuracy: 0.7079 - val_loss: 3.7294 - val_accuracy: 0.3333\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.2971 - accuracy: 0.9069 - val_loss: 4.1030 - val_accuracy: 0.3333\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5427 - accuracy: 0.9194 - val_loss: 7.4304 - val_accuracy: 0.2222\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.5295 - accuracy: 0.7264 - val_loss: 9.3281 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.8801 - accuracy: 0.8144 - val_loss: 5.8894 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.7624 - accuracy: 0.5278 - val_loss: 3.7416 - val_accuracy: 0.4444\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 1.6465 - accuracy: 0.5218 - val_loss: 4.9186 - val_accuracy: 0.1111\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.5037 - accuracy: 0.6394 - val_loss: 1.5162 - val_accuracy: 0.6667\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.7908 - accuracy: 0.9319 - val_loss: 2.8291 - val_accuracy: 0.4444\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 2.0028 - accuracy: 0.7949 - val_loss: 3.8183 - val_accuracy: 0.4444\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.7763 - accuracy: 0.8449 - val_loss: 1.7297 - val_accuracy: 0.5556\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5432 - accuracy: 0.8694 - val_loss: 4.4174 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.7206 - accuracy: 0.7454 - val_loss: 6.2650 - val_accuracy: 0.1111\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4164 - accuracy: 0.8199 - val_loss: 9.3827 - val_accuracy: 0.1111\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.2992 - accuracy: 0.8880 - val_loss: 3.8489 - val_accuracy: 0.3333\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.2505 - accuracy: 0.9444 - val_loss: 7.1054 - val_accuracy: 0.1111\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4754 - accuracy: 0.8944 - val_loss: 4.7031 - val_accuracy: 0.2222\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.4406 - accuracy: 0.8324 - val_loss: 6.6644 - val_accuracy: 0.2222\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.3806 - accuracy: 0.8264 - val_loss: 2.4644 - val_accuracy: 0.7778\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.3570 - accuracy: 0.9130 - val_loss: 1.2282 - val_accuracy: 0.6667\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.1203 - accuracy: 0.9505 - val_loss: 0.6480 - val_accuracy: 0.6667\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3636 - accuracy: 0.8319 - val_loss: 0.3899 - val_accuracy: 0.7778\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5422 - accuracy: 0.8449 - val_loss: 5.4380 - val_accuracy: 0.4444\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.1540 - accuracy: 0.9630 - val_loss: 2.6958 - val_accuracy: 0.6667\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.5754 - accuracy: 0.8759 - val_loss: 5.1543 - val_accuracy: 0.1111\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5284 - accuracy: 0.9194 - val_loss: 3.3935 - val_accuracy: 0.2222\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4260 - accuracy: 0.7704 - val_loss: 8.0726 - val_accuracy: 0.1111\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 10\n",
    "fitted_model = model.fit(train_X, train_y, batch_size=batch_size, epochs=50, validation_data=(validation_X, validation_y))\n",
    "#fitted_model = model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 27, 5, 64)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-4c813915f118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[1;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m       \u001b[1;31m# refresh its output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m       \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    967\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    970\u001b[0m                                                 input_list)\n\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1105\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1106\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1108\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    838\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    876\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2597\u001b[0m     \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2599\u001b[1;33m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[0;32m   2600\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[0;32m   2601\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[0;32m    216\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 27, 5, 64)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(64, (3), input_shape=np.shape(train_X)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(64, (2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "\n",
    "model.add(Conv1D(64, (2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}